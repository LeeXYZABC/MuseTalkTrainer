{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed5d164-8d80-43a6-9ee4-b51f28c92295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from musetalk.models.unet import UNet,PositionalEncoding\n",
    "\n",
    "# device=\"cuda\"\n",
    "\n",
    "# model_path = \"checkpoints/unet_checkpoint_step000380000.pth\"\n",
    "\n",
    "# model_bin_path = \"./cunet.bin\"\n",
    "# unet = UNet(unet_config=\"./models/musetalk/musetalk.json\", model_path=None, device=device, use_float16=True)\n",
    "# # gpu\n",
    "# weights = torch.load(model_path)[\"state_dict\"]\n",
    "# unet.model.load_state_dict(weights)\n",
    "\n",
    "# # cpu version\n",
    "# # weights = torch.load(model_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "# # unet.model.load_state_dict(weights)\n",
    "\n",
    "# torch.save(unet.model.state_dict(), model_bin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2ced5d-7902-4d9c-9112-cc066c8ad3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from musetalk.whisper.audio2feature import Audio2Feature\n",
    "from musetalk.models.vae import VAE\n",
    "from musetalk.models.unet import UNet,PositionalEncoding\n",
    "\n",
    "device=\"cuda\"\n",
    "use_float16=True\n",
    "\n",
    "audio_processor = Audio2Feature(model_path=\"./models/whisper/tiny.pt\", device=device)\n",
    "vae = VAE(model_path = \"./models/sd-vae-ft-mse/\", device=device, use_float16=True)\n",
    "\n",
    "model_bin_path = \"./cunet.bin\"\n",
    "unet = UNet(unet_config=\"./models/musetalk/musetalk.json\", model_path=model_bin_path, device=device, use_float16=True)\n",
    "\n",
    "pe = PositionalEncoding(d_model=384)\n",
    "timesteps = torch.tensor([0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e134126b-6961-4b41-920f-d2126fe1fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:08<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_list: 300, bbox_list: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from musetalk.utils.preprocessing import get_landmark_and_bbox_from_frames, coord_placeholder\n",
    "import cv2\n",
    "\n",
    "# video_file = \"data/video/sun.mp4\"\n",
    "video_file = \"/data/apps/MuseTalk/data/256/driver.mp4\"\n",
    "\n",
    "video_stream = cv2.VideoCapture(video_file)\n",
    "fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "frames = []\n",
    "while 1:\n",
    "    still_reading, frame = video_stream.read()\n",
    "    if not still_reading:\n",
    "        video_stream.release()\n",
    "        break\n",
    "    frames.append(frame.copy())\n",
    "\n",
    "bbox_shift = 0\n",
    "bbox_list, frame_list = get_landmark_and_bbox_from_frames(frames, bbox_shift)\n",
    "\n",
    "print(f\"frame_list: {len(frame_list)}, bbox_list: {len(bbox_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8566c4ad-74b3-405e-b7d1-93d994e4c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:06, 45.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def bbox_process(cords):\n",
    "    new_cords = []\n",
    "    for cord in cords:\n",
    "        x1, y1, x2, y2 = cord\n",
    "        x1 = max(x1, 0)\n",
    "        y1 = max(y1, 0)\n",
    "        new_cords.append((x1, y1, x2, y2))\n",
    "    return new_cords\n",
    "\n",
    "bbox_list = bbox_process(bbox_list)\n",
    "\n",
    "i = 0\n",
    "input_latent_list = []\n",
    "input_face_list = []\n",
    "for bbox, frame in tqdm(zip(bbox_list, frame_list)):\n",
    "    if bbox == coord_placeholder:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    crop_frame = frame[y1:y2, x1:x2]\n",
    "    crop_frame = cv2.resize(crop_frame,(256,256),interpolation = cv2.INTER_LANCZOS4)\n",
    "    input_face_list.append(crop_frame.copy())\n",
    "    latents = vae.get_latents_for_unet(crop_frame)\n",
    "    input_latent_list.append(latents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdc3506-75fa-4a88-829b-dc6222324ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video in 30.0 FPS, audio idx in 50FPS\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"data/audio/sun.wav\"\n",
    "whisper_feature = audio_processor.audio2feat(audio_path)\n",
    "whisper_chunks = audio_processor.feature2chunks(feature_array=whisper_feature,fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e49249-ec21-446c-8489-64daaed6b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [02:12<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from musetalk.utils.blending import get_image, get_image_prepare_material,get_image_blending\n",
    "\n",
    "batch_size = 1\n",
    "whisper_batch, latent_batch = [], []\n",
    "\n",
    "whisper_length = len(whisper_chunks)\n",
    "results = []\n",
    "for i in tqdm(range(0, whisper_length, batch_size)):\n",
    "    \n",
    "    audio_feature_batch = whisper_chunks[i:i+batch_size]\n",
    "    audio_feature_batch = np.stack(audio_feature_batch)\n",
    "    audio_feature_batch = torch.from_numpy(audio_feature_batch)\n",
    "\n",
    "    face_batch = [input_face_list[idx%(len(input_latent_list))] for idx in range(i, min(len(whisper_chunks), i+batch_size))]\n",
    "    face_batch = np.stack(face_batch)\n",
    "    \n",
    "    latent_batch = [input_latent_list[idx%(len(input_latent_list))] for idx in range(i, min(len(whisper_chunks), i+batch_size))]\n",
    "    latent_batch = torch.cat(latent_batch, dim=0)\n",
    "\n",
    "    audio_feature_batch = audio_feature_batch.to(device=unet.device, dtype=unet.model.dtype) # torch, B, 5*N,384\n",
    "    audio_feature_batch = pe(audio_feature_batch)\n",
    "    latent_batch = latent_batch.to(dtype=unet.model.dtype).to(unet.device)\n",
    "    \n",
    "    pred_latents = unet.model(latent_batch.half(), timesteps.half(), encoder_hidden_states=audio_feature_batch.half()).sample\n",
    "    \n",
    "    recon = vae.decode_latents(pred_latents)\n",
    "        \n",
    "    res_frame_list = []\n",
    "    for res_frame in recon:\n",
    "        res_frame_list.append(res_frame)\n",
    "\n",
    "    for offset, res_frame in enumerate(res_frame_list):\n",
    "        idx = i + offset\n",
    "        bbox = bbox_list[idx%(len(bbox_list))]\n",
    "        ori_frame = copy.deepcopy(frame_list[i%(len(frame_list))])\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\n",
    "        combine_frame = get_image(ori_frame, res_frame, bbox)\n",
    "        results.append(copy.deepcopy(combine_frame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb2de91-c2ba-4def-91ae-15c74d75c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfile = \"gen_test.mp4\"\n",
    "\n",
    "frame_h, frame_w = results[0].shape[:-1]\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(vfile, fourcc, fps, (frame_w, frame_h))\n",
    "for f in results:\n",
    "    out.write(f.astype(np.uint8))\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
